resources:

  pipelines:
    # DLT pipeline..
    source2_medium_metrics_pipeline:
      name: "[${bundle.environment}] FE Medium Metrics Pipeline - Volvo"
      target: "medium_post_report_${bundle.environment}"
      libraries:
        - file:
            path: ./ingest.py
        - file:
            path: ./get_metrics.py
      channel: preview
      configuration:
        "bundle.file_path": "/Workspace/${workspace.file_path}"

  jobs:
    # A two-task Databricks Workflow - dlt + notebook report
    source2_fe_medium_metrics:
      name: "[${bundle.environment}] Metrics for FE Medium Posts"
      tasks:
        - task_key: dlt_medium_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.medium_metrics_pipeline.id}
        - task_key: "${bundle.environment}_medium_notebook_report"
          depends_on:
            - task_key: dlt_medium_pipeline
          notebook_task:
            base_parameters:
              dbname: "medium_post_report_${bundle.environment}"
            notebook_path: ./fe_medium_report.py
          new_cluster:
            spark_version: 13.1.x-scala2.12
            num_workers: 1
            node_type_id: Standard_DS3_v2      