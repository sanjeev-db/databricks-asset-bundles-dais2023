resources:

  pipelines:
    # DLT pipeline..
    outbound_medium_metrics_pipeline:
      name: "[${bundle.environment}] outbound Medium Metrics Pipeline"
      target: "medium_post_report_${bundle.environment}"
      libraries:
        - file:
            path: ./outbound/ingest.py
        - file:
            path: ./outbound/get_metrics.py
      channel: preview
      configuration:
        "bundle.file_path": "/Workspace/${workspace.file_path}"

  jobs:
    # A two-task Databricks Workflow - dlt + notebook report
    outbound_fe_medium_metrics:
      name: "[${bundle.environment}] outbound Metrics for Medium Posts"
      tasks:
        - task_key: dlt_medium_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.medium_metrics_pipeline.id}
        - task_key: "${bundle.environment}_medium_notebook_report"
          depends_on:
            - task_key: dlt_medium_pipeline
          notebook_task:
            base_parameters:
              dbname: "medium_post_report_${bundle.environment}"
            notebook_path: ./outbound/fe_medium_report.py
          new_cluster:
            spark_version: ${var.spark_version}
            num_workers: 1
            node_type_id: ${var.node_type_id} 